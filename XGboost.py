#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Wed Feb 19 16:05:16 2020@authors: alanloret"""# import packagesimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom pandas_datareader import data as webimport datetime# machine learning packagesfrom xgboost import XGBRegressor# measure resultsfrom sklearn.metrics import mean_squared_error""" Collecting our data from Yahoo website """df_CAC40 = web.DataReader('^FCHI', 'yahoo', datetime.datetime(1985, 1, 1), datetime.datetime(2020, 1, 1))\              .rename(columns={'Adj Close': 'Adj_Close'})""" XGBoost :    We will use the XGradient Boosting module to predict the index """# Sorting and creating a dataframe with the target variabledf_CAC40 = df_CAC40.sort_index()data = df_CAC40.copy()# Crating featuresdata['1d_MinMax'] = data.High - data.Lowdata['1d_diff'] = data.Open - data.Closedata['returns'] = data.Close.pct_change()data['change'] = data.Close.diff()data['gain'] = data.change.mask(data.change < 0, 0.0)data['loss'] = -data.change.mask(data.change > 0, -0.0)data['3d_pct'] = data.Close.pct_change(3)for depth in [14, 18, 30]:    data['ma' + str(depth)] = data.Close.rolling(depth).mean()    data['rsi' + str(depth)] = 100 - 100/(1 + data.gain.rolling(depth).mean()/data.loss.rolling(depth).mean())# Target values :data['target'] = data['change'].shift(-1)# Drop all na valuesdata.dropna(axis=0, inplace=True)list_features = ['High', 'Low', 'Open', 'Close', 'Volume', 'Adj_Close', '1d_MinMax', '1d_diff', 'returns', 'change',                 'gain', 'loss', '3d_pct', 'ma14', 'rsi14', 'ma18', 'rsi18', 'ma30', 'rsi30']# Select the features we want to usefeatures = ['Low', 'Close', 'Volume', '1d_MinMax', '1d_diff', 'returns', 'change', 'gain', 'loss', '3d_pct', 'ma14',            'rsi14', 'ma18', 'rsi18', 'ma30', 'rsi30', 'target']df = data[features].copy()# Select our data for train/valid/testvalid_rows = 277  # 277 days just to compare with the same test sample of other modelstrain = df[:-2 * valid_rows]valid = df[-2 * valid_rows:-valid_rows]test = df[-valid_rows:]# Scale our data : the scale is per columnscale_min, scale_max = train.min(), train.max()train = (train - scale_min) / (scale_max - scale_min)valid = (valid - scale_min) / (scale_max - scale_min)test = (test - scale_min) / (scale_max - scale_min)X_train, y_train = train.loc[:, df.columns != 'target'], train.targetX_valid, y_valid = valid.loc[:, df.columns != 'target'], valid.targetX_test, y_test = test.loc[:, df.columns != 'target'], test.target"""List = []N = 100for i in range(1, N//5):    print('{:.2f} %'.format(100*(i - 1)/N*5))    for j in range(5, 20):        model = XGBRegressor(learning_rate=i/N, max_depth=j)        # Train the model using the training sets        model.fit(X_train, y_train)        # Get predictions        y_pred_valid = model.predict(X_valid) * (scale_max.target - scale_min.target) + scale_min.target        y_pred_test = model.predict(X_test) * (scale_max.target - scale_min.target) + scale_min.target        prediction = pd.DataFrame(y_pred_test, index=y_test.index, columns=['change'])        prediction = prediction * (scale_max.target - scale_min.target) + scale_min.target        results = pd.concat([prediction, y_test], axis=1)        accuracy = results.apply(lambda row: row.change * row.target >= 0, axis=1).value_counts()[True]        List.append([i/N, j, 100 * accuracy / valid_rows])List.sort(key=lambda x: x[2], reverse=True)"""# Define the modelmodel = XGBRegressor(learning_rate=0.17, max_depth=7)# Train the model using the training setsmodel.fit(X_train, y_train)# Get predictionsy_pred_valid = model.predict(X_valid) * (scale_max.target - scale_min.target) + scale_min.targety_pred_test = model.predict(X_test) * (scale_max.target - scale_min.target) + scale_min.targety_valid = y_valid * (scale_max.target - scale_min.target) + scale_min.targety_test = y_test * (scale_max.target - scale_min.target) + scale_min.target# Get accuracy ratesprediction = pd.DataFrame(y_pred_test, index=y_test.index, columns=['change'])prediction = prediction * (scale_max.target - scale_min.target) + scale_min.targetresults = pd.concat([prediction, y_test], axis=1)accuracy = results.apply(lambda row: row.change * row.target >= 0, axis=1).value_counts()[True]nbr_growth = results.apply(lambda row: row.target >= 0, axis=1).value_counts()[True]nbr_buy = results.apply(lambda row: row.change >= 0, axis=1).value_counts()[True]profit = results.apply(lambda row: (row.change >= 0) and (row.target >= 0), axis=1).value_counts()[True]nbr_sell = results.apply(lambda row: row.change < 0, axis=1).value_counts()[True]leave = results.apply(lambda row: (row.change < 0) and (row.target < 0), axis=1).value_counts()[True]# Results# Errorsprint('Accuracy of the model\n')print('RSME error valid data : ', np.sqrt(mean_squared_error(y_valid, y_pred_valid)))print('RSME error test data : ', np.sqrt(mean_squared_error(y_test, y_pred_test)))print('\nWell predicted the next variation : {:.2f} %'.format(100 * accuracy / valid_rows))print('Accuracy when we buy : {:.2f} %'.format(100 * profit / nbr_buy))print('Accuracy when we leave : {:.2f} %'.format(100 * leave / nbr_sell))print('Lost opportunities : {:.2f} %'.format(100 * (1 - profit / nbr_growth)))# Graphsplt.clf()plt.figure(figsize=(11, 7))plt.title('XGBoost')plt.scatter(y_valid, y_pred_valid, label='valid', color='r')plt.scatter(y_test, y_pred_test, label='test', color='blue')plt.xlabel('Actual')plt.ylabel('Predicted')plt.legend()plt.show()